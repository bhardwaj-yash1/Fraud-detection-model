{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c52cbab6",
   "metadata": {},
   "source": [
    "\n",
    "# Fraud Detection Assignment â€” Endâ€‘toâ€‘End Solution\n",
    "**Author:** _Your Name_  \n",
    "**Deadline:** 25 Aug, 11:59 PM IST\n",
    "\n",
    "This notebook is structured to satisfy all deliverables in the brief:\n",
    "\n",
    "1) **Data cleaning**: missing values, outliers, multicollinearity  \n",
    "2) **Model**: endâ€‘toâ€‘end build with rationale  \n",
    "3) **Feature selection rationale**  \n",
    "4) **Performance demonstration**: rigorous, timeâ€‘aware validation, metrics, plots  \n",
    "5) **Key fraud drivers** (global + local explainability)  \n",
    "6) **Sanity of factors** (business reasoning)  \n",
    "7) **Prevention recommendations**  \n",
    "8) **Measurement plan**\n",
    "\n",
    "> ðŸ”§ **How to use**\n",
    "> - Put your dataset path in the cell below (`DATA_PATH`).  \n",
    "> - Run cells in order.  \n",
    "> - If running on Colab, enable GPU/TPU (optional).  \n",
    "> - This notebook uses memoryâ€‘efficient reading + LightGBM/XGBoost (install as needed).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "477418be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.3.2-cp313-cp313-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\yashh\\fraud detection model\\fraud-env\\lib\\site-packages (2.2.6)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.7.1-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Collecting lightgbm\n",
      "  Using cached lightgbm-4.6.0-py3-none-win_amd64.whl.metadata (17 kB)\n",
      "Collecting xgboost\n",
      "  Using cached xgboost-3.0.4-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting shap\n",
      "  Using cached shap-0.48.0-cp313-cp313-win_amd64.whl.metadata (25 kB)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.5-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Collecting plotly\n",
      "  Using cached plotly-6.3.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\yashh\\fraud detection model\\fraud-env\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\yashh\\fraud detection model\\fraud-env\\lib\\site-packages (21.0.0)\n",
      "Collecting fastparquet\n",
      "  Using cached fastparquet-2024.11.0-cp313-cp313-win_amd64.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\yashh\\fraud detection model\\fraud-env\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\yashh\\fraud detection model\\fraud-env\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\yashh\\fraud detection model\\fraud-env\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\yashh\\fraud detection model\\fraud-env\\lib\\site-packages (from scikit-learn) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\yashh\\fraud detection model\\fraud-env\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\yashh\\fraud detection model\\fraud-env\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\users\\yashh\\fraud detection model\\fraud-env\\lib\\site-packages (from shap) (25.0)\n",
      "Requirement already satisfied: slicer==0.0.8 in c:\\users\\yashh\\fraud detection model\\fraud-env\\lib\\site-packages (from shap) (0.0.8)\n",
      "Collecting numba>=0.54 (from shap)\n",
      "  Using cached numba-0.61.2-cp313-cp313-win_amd64.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\yashh\\fraud detection model\\fraud-env\\lib\\site-packages (from shap) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\yashh\\fraud detection model\\fraud-env\\lib\\site-packages (from shap) (4.14.1)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.3-cp313-cp313-win_amd64.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\yashh\\fraud detection model\\fraud-env\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\yashh\\fraud detection model\\fraud-env\\lib\\site-packages (from matplotlib) (4.59.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\yashh\\fraud detection model\\fraud-env\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\yashh\\fraud detection model\\fraud-env\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\yashh\\fraud detection model\\fraud-env\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\yashh\\fraud detection model\\fraud-env\\lib\\site-packages (from plotly) (2.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\yashh\\fraud detection model\\fraud-env\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: cramjam>=2.3 in c:\\users\\yashh\\fraud detection model\\fraud-env\\lib\\site-packages (from fastparquet) (2.11.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\yashh\\fraud detection model\\fraud-env\\lib\\site-packages (from fastparquet) (2025.7.0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\yashh\\fraud detection model\\fraud-env\\lib\\site-packages (from numba>=0.54->shap) (0.44.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yashh\\fraud detection model\\fraud-env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached pandas-2.3.2-cp313-cp313-win_amd64.whl (11.0 MB)\n",
      "Using cached scikit_learn-1.7.1-cp313-cp313-win_amd64.whl (8.7 MB)\n",
      "Using cached lightgbm-4.6.0-py3-none-win_amd64.whl (1.5 MB)\n",
      "Using cached xgboost-3.0.4-py3-none-win_amd64.whl (56.8 MB)\n",
      "Using cached shap-0.48.0-cp313-cp313-win_amd64.whl (545 kB)\n",
      "Using cached matplotlib-3.10.5-cp313-cp313-win_amd64.whl (8.1 MB)\n",
      "Using cached plotly-6.3.0-py3-none-any.whl (9.8 MB)\n",
      "Using cached fastparquet-2024.11.0-cp313-cp313-win_amd64.whl (673 kB)\n",
      "Using cached contourpy-1.3.3-cp313-cp313-win_amd64.whl (226 kB)\n",
      "Using cached numba-0.61.2-cp313-cp313-win_amd64.whl (2.8 MB)\n",
      "Installing collected packages: plotly, numba, contourpy, xgboost, scikit-learn, pandas, matplotlib, lightgbm, shap, fastparquet\n",
      "Successfully installed contourpy-1.3.3 fastparquet-2024.11.0 lightgbm-4.6.0 matplotlib-3.10.5 numba-0.61.2 pandas-2.3.2 plotly-6.3.0 scikit-learn-1.7.1 shap-0.48.0 xgboost-3.0.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages -> LGBM: True | XGB: True | SHAP: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# If running locally and missing packages, uncomment installs:\n",
    "%pip install pandas numpy scikit-learn lightgbm xgboost shap matplotlib plotly tqdm pyarrow fastparquet\n",
    "\n",
    "import os, gc, math, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, precision_recall_curve,\n",
    "    confusion_matrix, classification_report, roc_curve\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional (if available):\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    HAS_LGBM = True\n",
    "except Exception:\n",
    "    HAS_LGBM = False\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    HAS_XGB = True\n",
    "except Exception:\n",
    "    HAS_XGB = False\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "    HAS_SHAP = True\n",
    "except Exception:\n",
    "    HAS_SHAP = False\n",
    "\n",
    "print(\"Packages -> LGBM:\", HAS_LGBM, \"| XGB:\", HAS_XGB, \"| SHAP:\", HAS_SHAP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c253202",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====== CONFIG ======\n",
    "DATA_PATH = \"C:\\Users\\yashh\\Downloads\\Fraud.csv\"  # <- <-- PUT YOUR CSV PATH HERE\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.20  # time-aware split is applied below; this is a fallback\n",
    "TARGET_COL = \"isFraud\"\n",
    "\n",
    "# Memory optimization: set dtypes for known columns\n",
    "DTYPES = {\n",
    "    \"step\": \"int32\",\n",
    "    \"type\": \"category\",\n",
    "    \"amount\": \"float32\",\n",
    "    \"nameOrig\": \"category\",\n",
    "    \"oldbalanceOrg\": \"float32\",\n",
    "    \"newbalanceOrig\": \"float32\",\n",
    "    \"nameDest\": \"category\",\n",
    "    \"oldbalanceDest\": \"float32\",\n",
    "    \"newbalanceDest\": \"float32\",\n",
    "    \"isFraud\": \"int8\",\n",
    "    \"isFlaggedFraud\": \"int8\",\n",
    "}\n",
    "\n",
    "assert os.path.exists(DATA_PATH), f\"Dataset not found at {DATA_PATH}. Please update DATA_PATH.\"\n",
    "print('Using dataset at:', DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8397175",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====== LOAD DATA ======\n",
    "# For 6.36M rows, pandas can load with the right dtypes on a 16GB+ machine.\n",
    "# If RAM is tight, consider reading with chunks for EDA; we do full load for modeling.\n",
    "df = pd.read_csv(DATA_PATH, dtype=DTYPES)\n",
    "print(df.shape)\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42c9e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====== DATA HEALTH CHECKS ======\n",
    "print(\"\\nBasic Info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nTarget balance:\")\n",
    "print(df[TARGET_COL].value_counts(normalize=True).rename('proportion'))\n",
    "\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "# Summary stats for numeric columns\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = df.select_dtypes(include=['category', 'object']).columns.tolist()\n",
    "print(\"\\nNumeric columns:\", num_cols)\n",
    "print(\"Categorical columns:\", cat_cols)\n",
    "\n",
    "df[num_cols].describe(percentiles=[.01,.05,.25,.5,.75,.95,.99])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4e8482",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (target leakage, distributions, imbalances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618a8e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Target vs type\n",
    "ct = pd.crosstab(df['type'], df[TARGET_COL], normalize='index')\n",
    "print(ct)\n",
    "\n",
    "# Distribution of amounts (trimmed)\n",
    "amt_q99 = df['amount'].quantile(0.99)\n",
    "df['amount_clip'] = df['amount'].clip(upper=amt_q99)\n",
    "df['amount_clip'].hist(bins=50)\n",
    "plt.title('Amount (clipped at 99th pct)')\n",
    "plt.xlabel('amount')\n",
    "plt.ylabel('count')\n",
    "plt.show()\n",
    "\n",
    "# Time progression of fraud rate\n",
    "fraud_rate_by_step = df.groupby('step')[TARGET_COL].mean()\n",
    "fraud_rate_by_step.plot()\n",
    "plt.title('Fraud rate by time step (hour)')\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('fraud_rate')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237fb0ee",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a4c739",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Derived risk features\n",
    "df['is_type_TRANSFER'] = (df['type'] == 'TRANSFER').astype('int8')\n",
    "df['is_type_CASH_OUT'] = (df['type'] == 'CASH_OUT').astype('int8')\n",
    "df['is_merchant_dest'] = df['nameDest'].astype(str).str.startswith('M').astype('int8')\n",
    "\n",
    "# Balance deltas for origin\n",
    "df['deltaOrig'] = (df['oldbalanceOrg'] - df['newbalanceOrig']).astype('float32')\n",
    "df['deltaDest'] = (df['newbalanceDest'] - df['oldbalanceDest']).astype('float32')\n",
    "\n",
    "# Suspicious patterns\n",
    "df['orig_balance_zero_then_txn'] = ((df['oldbalanceOrg'] == 0) & (df['amount'] > 0)).astype('int8')\n",
    "df['dest_balance_zero_then_in'] = ((df['oldbalanceDest'] == 0) & (df['amount'] > 0)).astype('int8')\n",
    "df['mismatch_orig'] = (np.abs(df['deltaOrig'] - df['amount']) > 1e-2).astype('int8')\n",
    "df['mismatch_dest'] = (np.abs(df['deltaDest'] - df['amount']) > 1e-2).astype('int8')\n",
    "\n",
    "# Drop leakage columns (IDs kept only if used as categorical signals)\n",
    "LEAKS = []  # if any discovered, append here\n",
    "feature_cols = [c for c in df.columns if c not in [TARGET_COL, 'isFlaggedFraud', 'amount_clip'] + LEAKS]\n",
    "print(\"Feature count:\", len(feature_cols))\n",
    "feature_cols[:15]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72e8b4f",
   "metadata": {},
   "source": [
    "## Train/Validation Split (Timeâ€‘aware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf30b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Time-aware split: use first 80% steps for train, last 20% for validation\n",
    "step_cut = int(df['step'].quantile(0.80))\n",
    "train_idx = df['step'] <= step_cut\n",
    "valid_idx = df['step'] > step_cut\n",
    "\n",
    "train = df.loc[train_idx].reset_index(drop=True)\n",
    "valid = df.loc[valid_idx].reset_index(drop=True)\n",
    "\n",
    "X_train = train[feature_cols]\n",
    "y_train = train[TARGET_COL].astype('int8')\n",
    "X_valid = valid[feature_cols]\n",
    "y_valid = valid[TARGET_COL].astype('int8')\n",
    "\n",
    "print(train.shape, valid.shape, \" | step_cut:\", step_cut)\n",
    "\n",
    "# Encode categoricals (simple): convert category to codes\n",
    "for c in X_train.select_dtypes(include=['category']).columns:\n",
    "    # Ensure consistent codes between train/valid\n",
    "    allcats = pd.Categorical(df[c])\n",
    "    cat2code = {cat: i for i, cat in enumerate(allcats.categories)}\n",
    "    X_train[c] = pd.Categorical(X_train[c], categories=allcats.categories).codes.astype('int32')\n",
    "    X_valid[c] = pd.Categorical(X_valid[c], categories=allcats.categories).codes.astype('int32')\n",
    "\n",
    "# Fill any remaining NaNs (should be rare)\n",
    "X_train = X_train.fillna(0)\n",
    "X_valid = X_valid.fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59afd1e9",
   "metadata": {},
   "source": [
    "## Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c8ad77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_preds(y_true, y_prob, threshold=0.5, title_suffix=\"\"):\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    ap = average_precision_score(y_true, y_prob)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(f\"AUC: {auc:.5f} | Average Precision (PR AUC): {ap:.5f}\")\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    print(\"\\nClassification report:\\n\", classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "    # PR Curve\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    plt.figure()\n",
    "    plt.plot(recall, precision)\n",
    "    plt.title(f'Precision-Recall Curve {title_suffix}')\n",
    "    plt.xlabel('Recall'); plt.ylabel('Precision')\n",
    "    plt.show()\n",
    "\n",
    "    # ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.title(f'ROC Curve {title_suffix}')\n",
    "    plt.xlabel('FPR'); plt.ylabel('TPR')\n",
    "    plt.show()\n",
    "\n",
    "# 8.1 LightGBM (preferred for tabular & imbalance with class_weight)\n",
    "if HAS_LGBM:\n",
    "    lgb_train = lgb.Dataset(X_train, label=y_train)\n",
    "    lgb_valid = lgb.Dataset(X_valid, label=y_valid, reference=lgb_train)\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': ['auc'],\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 64,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 2,\n",
    "        'reg_lambda': 5.0,\n",
    "        'min_data_in_leaf': 50,\n",
    "        'max_depth': -1,\n",
    "        'verbose': -1,\n",
    "        'scale_pos_weight': max(1.0, (y_train==0).sum() / max(1,(y_train==1).sum())),\n",
    "        'seed': 42\n",
    "    }\n",
    "    print(\"Training LightGBM with params:\", params)\n",
    "    lgb_model = lgb.train(\n",
    "        params,\n",
    "        lgb_train,\n",
    "        valid_sets=[lgb_train, lgb_valid],\n",
    "        valid_names=['train','valid'],\n",
    "        num_boost_round=2000,\n",
    "        early_stopping_rounds=100,\n",
    "        verbose_eval=100\n",
    "    )\n",
    "    y_valid_prob_lgb = lgb_model.predict(X_valid, num_iteration=lgb_model.best_iteration)\n",
    "    evaluate_preds(y_valid, y_valid_prob_lgb, title_suffix=\"(LightGBM)\")\n",
    "else:\n",
    "    print(\"LightGBM not available; skipping.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956947ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if HAS_XGB:\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "    scale_pos_weight = max(1.0, (y_train==0).sum() / max(1,(y_train==1).sum()))\n",
    "    xgb_params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'eta': 0.05,\n",
    "        'max_depth': 8,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'lambda': 5.0,\n",
    "        'scale_pos_weight': scale_pos_weight,\n",
    "        'seed': 42\n",
    "    }\n",
    "    print(\"Training XGBoost with params:\", xgb_params)\n",
    "    xgb_model = xgb.train(\n",
    "        xgb_params,\n",
    "        dtrain,\n",
    "        num_boost_round=3000,\n",
    "        evals=[(dtrain,'train'),(dvalid,'valid')],\n",
    "        early_stopping_rounds=100,\n",
    "        verbose_eval=200\n",
    "    )\n",
    "    y_valid_prob_xgb = xgb_model.predict(dvalid, iteration_range=(0, xgb_model.best_ntree_limit))\n",
    "    evaluate_preds(y_valid, y_valid_prob_xgb, title_suffix=\"(XGBoost)\")\n",
    "else:\n",
    "    print(\"XGBoost not available; skipping.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6486f6",
   "metadata": {},
   "source": [
    "## Explainability â€” Feature Importance & SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f7822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Feature importances (model-dependent)\n",
    "def plot_importance(names, importances, topn=25, title=\"Feature Importance\"):\n",
    "    order = np.argsort(importances)[::-1][:topn]\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.barh(range(len(order)), np.array(importances)[order][::-1])\n",
    "    plt.yticks(range(len(order)), np.array(names)[order][::-1])\n",
    "    plt.title(title)\n",
    "    plt.xlabel('importance')\n",
    "    plt.show()\n",
    "\n",
    "if HAS_LGBM:\n",
    "    imp = lgb_model.feature_importance(importance_type='gain')\n",
    "    plot_importance(X_train.columns, imp, title=\"LightGBM Feature Importance (gain)\")\n",
    "\n",
    "# SHAP (optional, can be heavy)\n",
    "if HAS_SHAP and HAS_LGBM:\n",
    "    explainer = shap.TreeExplainer(lgb_model)\n",
    "    # Use a small sample for speed\n",
    "    sample = X_valid.sample(n=min(5000, len(X_valid)), random_state=RANDOM_STATE)\n",
    "    shap_values = explainer.shap_values(sample)\n",
    "    shap.summary_plot(shap_values, sample, show=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341df327",
   "metadata": {},
   "source": [
    "## Multicollinearity (VIF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23982ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute VIF on numeric subset to check multi-collinearity\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.api as sm\n",
    "\n",
    "num_for_vif = X_train.select_dtypes(include=[np.number]).copy()\n",
    "# Limit to a reasonable subset to keep runtime manageable\n",
    "cols_for_vif = [c for c in num_for_vif.columns if num_for_vif[c].nunique() > 10]\n",
    "cols_for_vif = cols_for_vif[:30]  # cap for speed\n",
    "vif_df = pd.DataFrame({\n",
    "    'feature': cols_for_vif,\n",
    "    'VIF': [variance_inflation_factor(num_for_vif[cols_for_vif].values, i) for i in range(len(cols_for_vif))]\n",
    "})\n",
    "vif_df.sort_values('VIF', ascending=False).head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6fbe91",
   "metadata": {},
   "source": [
    "## Threshold Tuning (Optimize for Business Cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c01931",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: choose threshold maximizing F1 or desired precision\n",
    "def best_threshold_by_precision_target(y_true, y_prob, precision_target=0.95):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    for p, r, t in zip(precision, recall, np.append(thresholds, 1)):\n",
    "        if p >= precision_target:\n",
    "            return float(t), float(p), float(r)\n",
    "    return 0.5, precision[0], recall[0]\n",
    "\n",
    "if HAS_LGBM:\n",
    "    thr, p, r = best_threshold_by_precision_target(y_valid, y_valid_prob_lgb, precision_target=0.90)\n",
    "    print(f\"Threshold for >=90% precision: {thr:.4f} -> precision={p:.3f}, recall={r:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ab0503",
   "metadata": {},
   "source": [
    "## Save Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e32027",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save predictions for audit / attachment in submission\n",
    "if HAS_LGBM:\n",
    "    valid_out = valid[['step','type','amount','nameOrig','nameDest','isFlaggedFraud',TARGET_COL]].copy()\n",
    "    valid_out['fraud_prob_lgb'] = y_valid_prob_lgb\n",
    "    valid_out.to_parquet('valid_predictions.parquet', index=False)\n",
    "    print(\"Saved: valid_predictions.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4604c2e3",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusions & Answers (fill after running)\n",
    "\n",
    "**1) Data cleaning:**  \n",
    "- Missing: _<notes>_  \n",
    "- Outliers: _<notes>_  \n",
    "- Multicollinearity: _<notes>_  \n",
    "\n",
    "**2) Model description:**  \n",
    "- Algorithm: _<LightGBM/XGBoost>_  \n",
    "- Why: _<reasoning>_  \n",
    "- Handling imbalance: _<scale_pos_weight, threshold tuning>_  \n",
    "\n",
    "**3) Variable selection:**  \n",
    "- Included: engineered deltas, type indicators, merchant flags, mismatch signals  \n",
    "- Rationale: information value, importance, SHAP  \n",
    "\n",
    "**4) Performance:**  \n",
    "- Metrics: AUC, PRâ€‘AUC, Precision@Recall, confusion matrix  \n",
    "- Validation: timeâ€‘aware split (first 80% steps train, last 20% validate)  \n",
    "\n",
    "**5) Key predictors:**  \n",
    "- Top features: _<from gain importance / SHAP>_  \n",
    "\n",
    "**6) Do they make sense?**  \n",
    "- Business reasoning: _<explain link to fraud modus operandi>_  \n",
    "\n",
    "**7) Prevention recommendations:**  \n",
    "- _<rate limiting highâ€‘risk flows, MFA, velocity rules, beneficiary cooling periods, anomaly detection in balances, merchant monitoring, graph rules, device fingerprinting>_  \n",
    "\n",
    "**8) Measurement of success:**  \n",
    "- A/B or backtest: reduction in fraud loss, false positive rate, precision/recall lift, alert fatigue drop, manual review SLA, net profit impact.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraud-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
